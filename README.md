# ğŸ—ï¸ pipeline-databricks-airflow

Este repositÃ³rio faz parte de um estudo prÃ¡tico que implementa um pipeline de ETL utilizando **Apache Airflow**, **Azure Databricks** e integraÃ§Ã£o com **Slack**, com o objetivo de extrair, transformar e reportar dados de cÃ¢mbio.

> âš ï¸ **Aviso:** Este projeto estÃ¡ hospedado como portfÃ³lio e depende de conexÃµes configuradas (Databricks e Slack) para execuÃ§Ã£o completa.

---

## ğŸ’¡ Objetivo

Construir um pipeline de dados que:

1. **Extrai dados de cÃ¢mbio** diariamente atravÃ©s da API Exchange Rates (USD, EUR, GBP em relaÃ§Ã£o ao BRL).
2. Realiza o **processamento e padronizaÃ§Ã£o** dos dados com notebooks no Databricks.
3. **Gera e envia relatÃ³rios automatizados** com dados e grÃ¡ficos via Slack, todos os dias Ã s 9h.

---

## ğŸ› ï¸ Tecnologias e Ferramentas Utilizadas

- **Apache Airflow**: OrquestraÃ§Ã£o das tarefas via DAGs.
- **Azure Databricks**: ExecuÃ§Ã£o de notebooks em PySpark.
- **Slack API**: Envio automatizado de relatÃ³rios.
- **Python + PySpark**
- **DBFS (Databricks File System)**: Armazenamento intermediÃ¡rio dos dados.
- **Linux com Virtualenv**: Ambiente de desenvolvimento.

---

## ğŸ“ Estrutura do RepositÃ³rio

```
â”œâ”€â”€ dags/                       # DAGs utilizadas no Airflow
â”‚   â””â”€â”€ extract-tax.py
â”œâ”€â”€ notebooks/                  # Notebooks executados no Databricks
â”‚   â”œâ”€â”€ 1-extract-data.py
â”‚   â”œâ”€â”€ 2-data-transform.py
|   â”œâ”€â”€ 2.1-data-transform-to-pandas.py # arquivo teste nÃ£o utilizado no job, sÃ³ convertido o tratamento para pandas spark
â”‚   â””â”€â”€ 3-automating-report.py
â”œâ”€â”€ imagens/                    # GrÃ¡ficos gerados e enviados via Slack
â”‚   â”œâ”€â”€ USD.png
â”‚   â”œâ”€â”€ EUR.png
â”‚   â””â”€â”€ GBP.png
â”œâ”€â”€ requirements.txt            # DependÃªncias do projeto
â”œâ”€â”€ README.md                   # Este arquivo
â””â”€â”€ .gitignore
```

---

## ğŸ”„ Fluxo do Pipeline

1. A **DAG no Airflow** Ã© agendada para rodar todos os dias Ã s 09:00.
2. A tarefa `extract_data` extrai dados de cÃ¢mbio da API e salva no DBFS (camada Bronze).
3. A tarefa `transform_data` realiza padronizaÃ§Ãµes e conversÃµes, salvando na camada Silver.
4. A tarefa `send_report`:
   - Gera grÃ¡ficos de evoluÃ§Ã£o para cada moeda (USD, EUR, GBP).
   - Envia para o Slack uma tabela com os valores tratados e os grÃ¡ficos correspondentes.

---

## ğŸ“Œ ObservaÃ§Ãµes

- Os grÃ¡ficos sÃ£o gerados dinamicamente e salvos na pasta `imagens/` antes de serem enviados.
- A integraÃ§Ã£o com o Slack utiliza tokens pessoais e webhooks (nÃ£o incluÃ­dos no repositÃ³rio).
- Cada notebook Ã© executado via `DatabricksRunNowOperator`, referenciando o `job_id` correspondente para cada JOB no Databricks.

---

## ğŸ‘¤ Autor

- [Gustavo Ribeiro](https://github.com/queirozene)
- [LinkedIn](https://www.linkedin.com/in/guxtavoqr/)
